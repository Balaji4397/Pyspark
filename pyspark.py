# -*- coding: utf-8 -*-
"""Pyspark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/160TtweLVAjToRMV-6kfFAruKyjm990Nz
"""

pip install pyspark

import pyspark
from pyspark.sql import SparkSession,functions as f
from pyspark.sql.types import *
from pyspark.sql.functions import monotonically_increasing_id, row_number
from pyspark.sql import Window

sc = SparkSession.Builder().appName('sample').master('local[5]').getOrCreate()
data = [
    (1, "Gilbert Gathara", "24", "Nairobi"),
    (2, "Someone Else", "32", "Nowhere")
]
headers = ("id", "Name", "Age", "Location")
df = sc.createDataFrame(data, headers)
df.show()
df=df.drop('Location')
df.show()
data2 = [[3,'Balaji',27],[4,'srikar',31]]
df2=sc.createDataFrame(data2, ["id", "Name", "Age"])
df2.show()
df = df.union(df2)
df.show()
location=['USA','Europe','India','India']
loc_df = sc.createDataFrame([(l,) for l in location], ['location'])
loc_df.show()
a = df.withColumn("x", row_number().over(Window.orderBy(monotonically_increasing_id())))
b = loc_df.withColumn("x", row_number().over(Window.orderBy(monotonically_increasing_id())))

final_df = a.join(b, a.x == b.x).drop("x")
final_df.show()

import pyspark
from pyspark.sql import *
from pyspark.sql.types import *
sc=SparkSession.Builder().appName('emptydataframe').getOrCreate()
schema = StructType([
    StructField('S.no',IntegerType(),True),
    StructField('Name',StringType(),True),
    StructField('Age', IntegerType(),True)
])
df = sc.createDataFrame([],schema)
df.show()
data = [(1,'balaji',26),(2,'srikar',31),(3,'sam',39)]
headers = ('S.no','Name','Age')
join_df = sc.createDataFrame(data, headers)
df = df.union(join_df)
df.show()
location=['lafayette','BR','BR']
loc_df = sc.createDataFrame([(l,) for l in location],['location'])
loc_df.show()
df = df.withColumn('x',row_number().over(Window.orderBy(monotonically_increasing_id())))
loc_df = loc_df.withColumn('x',row_number().over(Window.orderBy(monotonically_increasing_id())))
df = df.join(loc_df, df.x == loc_df.x).drop('x')
df.show()

import pyspark
from pyspark.sql import *
from pyspark.sql.types import *

sc= SparkSession.Builder().appName('Sample').master('local[5]').getOrCreate()
schema = StructType([
    StructField('ID', IntegerType(),True),
    StructField('Name', StringType(),True),
    StructField('Age', IntegerType(),True)
])
emptydf = sc.createDataFrame([],schema)
data = [(1,'balaji',26),(2,'srikar',31),(3,'sam',39)]
headers = ('ID','Name','Age')
df=sc.createDataFrame(data, headers)
df.show()
df.printSchema()
df = df.withColumn("ID", df["ID"].cast(IntegerType()))
df.printSchema()
location=['CA','LA','LA']
loc_df=sc.createDataFrame([(l,) for l in location],['location'])
loc_df.show()
df = df.withColumn('x',row_number().over(Window.orderBy(monotonically_increasing_id())))
loc_df = loc_df.withColumn('x',row_number().over(Window.orderBy(monotonically_increasing_id())))
df = df.join(loc_df, df.x==loc_df.x).drop('x')
df.show()
df=df.withColumnRenamed('location','state')
df.show()
df2=sc.createDataFrame([[4,'ranga','31','YV']])
df = df.union(df2)
df.show()

import pyspark
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql.functions import *

sc = SparkSession.Builder().appName('sample').master('local[5]').getOrCreate()
data = [(1,'balaji',26,'Lafayette'),(2,'srikar',31,'BR'),(3,'sam',39,'BR')]
headers = ('ID','Name','Age')
schema = StructType([
    StructField('ID',IntegerType(),True),
    StructField('Name',StringType(),True),
    StructField('Age',StringType(),True),
    StructField('Location',StringType(),True)
])
final_df = sc.createDataFrame(data, schema = schema)
final_df.show()
company_lst = ['CGI','Apple','Bayleaf']
comp_df = sc.createDataFrame([(l,) for l in company_lst],['Company'])
a = final_df.withColumn('x', row_number().over(Window.orderBy(monotonically_increasing_id())))
b = comp_df.withColumn('x', row_number().over(Window.orderBy(monotonically_increasing_id())))
df = a.join(b, a.x == b.x).drop('x')
df.show()
data2 = [[4,'Ranga',32,'YV','Non-IT']]
data2_df = sc.createDataFrame(data2,schema=df.schema)
data2_df.show()
df = df.union(data2_df)
df.show()
df = df.withColumnRenamed('Location','Address')
df.show()